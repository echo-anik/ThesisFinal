# Comprehensive Preprocessing Pipeline for WADI A2 Dataset: Hierarchical IDS in IIoT with Data Leakage Prevention

The WADI A2 dataset presents unique challenges for Industrial IoT intrusion detection systems, requiring rigorous preprocessing protocols to prevent data leakage while maintaining detection performance. This research synthesizes practices from recent literature (2020-2025) to provide actionable guidance for researchers and practitioners deploying anomaly detection systems on resource-constrained industrial infrastructure.

## WADI A2 dataset structure reveals critical preprocessing requirements

The WADI (Water Distribution) A2 dataset, released November 19, 2019 by iTrust Centre at Singapore University of Technology and Design, represents a scaled-down but high-fidelity water distribution system with **123 sensors and actuators** operating at **1-second sampling intervals**. The dataset captures 16 days of continuous operation: **789,371 samples** across 14 days of normal operation for training, and **172,801 samples** over 2 days containing 15 distinct attack scenarios for testing.

**Critical version distinction**: WADI A2 removed unstable periods from the original A1 release, reducing total samples by approximately 35% but providing cleaner, more reliable data. Recent research exclusively uses A2, making it the recommended version for new studies.

**Sensor taxonomy breakdown**: The 123 features include 18 Analyzer Indicator Transmitters (continuous), 5 Flow Indication Transmitters (continuous), 4 Level Transmitters (continuous), 3 Pressure Meters (continuous), 14 Pumps (discrete), 20 Motorized Valves (discrete), and 6 Solenoid Valves (discrete). The system architecture spans three operational stages: P1-W (primary supply with chemical dosing), P2-W (distribution to six consumer tanks with leak detection), and P3-W (return/recycling). Understanding this physical process structure informs intelligent feature engineering and domain-specific anomaly detection strategies.

**Attack scenario diversity**: The 15 attacks range from 2 to 30 minutes duration, encompassing valve manipulation, sensor spoofing, unauthorized pump activation causing pipe bursts, water quality contamination, level manipulation, stealthy gradual attacks, supply disruption, and artificial leakage creation. The **5.77% anomaly rate** in test data creates severe class imbalance requiring specialized evaluation metrics beyond simple accuracy.

**Critical preprocessing note**: Research consistently recommends **removing the first 21,600 samples** (approximately 6 hours) from training data to eliminate system stabilization artifacts when the testbed first activates. This preprocessing step appears in virtually all state-of-the-art papers and prevents models from learning initialization transients rather than normal operational patterns.

## Data leakage mechanisms in WADI require vigilant prevention strategies

**Temporal distribution leakage** represents the most pervasive challenge in WADI preprocessing. Statistical analysis using Kolmogorov-Smirnov tests reveals **15 sensors fail distribution consistency checks** between training and test sets even during labeled "normal" periods. These sensors exhibit different statistical characteristics due to system configuration changes, sensor degradation over time, post-attack instability not properly labeled, or environmental condition variations. Research identifies specific problematic sensors: 1_AIT_001_PV, 1_AIT_003_PV, 1_AIT_004_PV, 1_AIT_005_PV, 2_LT_001_PV, 2_PIT_001_PV, and eight analyzer transmitters (2A_AIT and 2B_AIT series), plus 3_AIT_005_PV. Papers achieving F1-scores above 0.60 consistently remove these 14 features after K-S testing on validation data.

**Constant feature trap**: Six solenoid valves (2_SV_101_STATUS through 2_SV_601_STATUS) maintain identical values throughout both training and test sets, providing zero information for anomaly detection. While some papers retain them for theoretical "attack surface coverage," lightweight deployments should eliminate these features to reduce dimensionality by 5% without accuracy loss.

**Normalization-induced leakage** remains the most common methodological error. The correct protocol requires fitting scalers exclusively on training data, then transforming validation and test sets using those training-derived parameters. Calculating normalization statistics from combined datasets allows models to learn future information patterns. Research implementing StandardScaler or MinMaxScaler consistently emphasizes: `scaler.fit(X_train)` followed by `scaler.transform(X_val)` and `scaler.transform(X_test)` using the **same training statistics** for all transformations.

**Feature selection leakage** occurs when researchers perform K-S tests or correlation analysis on test data to decide which features to remove. The proper methodology applies these statistical tests on training versus validation sets only, never touching test data. State-of-the-art papers like Kravchik & Shabtai (2021) explicitly document this protocol, achieving 75.40% F1-score on WADI through rigorous separation.

**Window creation leakage** in time-series data manifests when sliding windows span the training-test boundary, allowing prediction windows to include both training observations and test targets. Best practice creates windows independently within each dataset partition, ensuring no temporal window crosses split boundaries. For WADI with common window sizes of 4-20 samples, researchers must verify that the final training window ends before the first test window begins, maintaining strict temporal causality.

**State-based leakage** emerges from novel actuator configurations. Analysis reveals training and test sets share only 79-83% of system states (unique combinations of actuator values). Test data contains actuator configurations never observed during training, potentially causing false alarms when models encounter these unseen states. While difficult to eliminate entirely, state-aware feature engineering incorporating physical constraints can mitigate this issue.

## Recent literature converges on standardized preprocessing protocols

**Normalization techniques** show strong consensus toward **MinMaxScaler** as the most frequently used approach across papers from 2020-2025. InterFusion (Li et al., KDD 2021), MAD-GAN (Li et al., 2019), and multiple recent implementations apply MinMaxScaler fitted on training data with the formula `X_scaled = (X - X_min) / (X_max - X_min)`, scaling features to [0,1] range. This approach proves particularly effective for WADI's heterogeneous sensor types mixing flow rates, pressure readings, and binary actuator states.

**StandardScaler** (Z-score normalization) appears as secondary choice, used in Lightweight LSTM-VAE (Faber et al., 2022, PMC9030796) and MTAD-GAT implementations. The formula `X_scaled = (X - μ) / σ` centers features around zero with unit variance, beneficial when subsequent architectures like LSTMs expect zero-centered inputs. Critical implementation detail: test data normalization must use training mean and variance, never test statistics.

**QuantileTransformer** represents an emerging approach for WADI's high-definition sensors with very large value ranges. POLITECNICO DI TORINO Master's Thesis (2024) documents using QuantileTransformer with output distribution U(0,1) or N(0,1) to mitigate extreme values and outliers. While this naturally alters data distribution, it can improve robustness for sensors with occasional extreme readings during attacks. The tradeoff is reduced interpretability of normalized values.

**Missing value handling** requires special attention for WADI. The dataset contains extensive NaN values that cause training failures if unaddressed. The USAD implementation (GitHub: elisejiuqizhang/USAD-on-WADI-and-SWaT) documents that "WADI dataset is extremely big and contains a lot of missing values...these NaNs will affect the training of the model (so you would get NaN loss for every epoch)." The pragmatic solution: (1) identify and remove columns with no valid entries, (2) replace remaining NaNs with zeros or use forward-fill for temporal continuity, (3) verify no NaNs remain before training. Alternative approaches include mean/median imputation using training statistics only, or linear interpolation preserving temporal smoothness.

**Window size selection** shows interesting patterns across architectures. STADN (Tang et al., 2023, PMC10020568) achieving **F1=0.62** (current state-of-the-art on WADI) uses **w=20** samples with k=40 nearest neighbors for graph attention. MTAD-GAT experiments with 2, 4, 8, 16, 32 samples found **w=4** optimal for their architecture. MAD-GAN tested extensive ranges from 30 to 300 timesteps, noting that "the model becomes slow when the sub-sequence length sw is larger than 200." The consensus: **w=4 to w=20** provides the sweet spot for WADI, balancing temporal context capture against computational efficiency. Longer windows increase memory requirements quadratically and slow training substantially.

**Downsampling strategy** emerges as practical solution for computational constraints. Multiple papers apply **downsampling factor of 5**, reducing WADI from 1-second to 5-second intervals. This reduces training samples from ~790,000 to ~158,000 while maintaining sufficient temporal resolution for attack detection given that WADI attacks last 2-30 minutes. The USAD implementation explicitly documents: "WADI is huge and the processing takes a lot of memory" as justification for this approach.

**Outlier treatment** appears less frequently but shows promise. The Spectral Residual (SR) algorithm applies Fourier transform to identify anomalous points in normal training data through spectral analysis. Process: compute F(x), calculate log amplitude l = log(r), smooth using moving average, compute residual δ = l - l_smooth, inverse transform to get SR = |F^(-1)(e^(δ+iθ))|. Points exceeding threshold τ are replaced with mode. This cleans training data of occasional sensor glitches without removing entire features.

## Feature engineering techniques enhance detection while enabling dimensionality reduction

**Statistical window features** represent the foundational approach, computing mean, standard deviation, minimum, and maximum over sliding windows. MAD-GAN tested window sizes from 30 to 300 timesteps, finding shorter windows (30-120 samples) perform better for cyber-physical systems with quick response dynamics. These features transform raw sensor streams into summarized representations capturing local patterns while reducing dimensionality. For WADI's 112 post-filtered features with w=30 windows, this creates 448 statistical features as input to anomaly detection models.

**Temporal lag features** explicitly capture historical dependencies through features like x(t-1), x(t-2), ... x(t-n). However, LSTM and GRU architectures naturally model these dependencies through recurrent connections, making explicit lag feature engineering often unnecessary when using temporal neural networks. Papers using LSTM-based architectures typically skip manual lag feature creation, allowing the network's hidden state to capture temporal patterns automatically.

**Spatial dependency modeling** through Graph Neural Networks represents recent breakthrough approaches. STADN constructs adjacency matrices using correlation coefficients between sensor pairs, selecting **top-k=40 neighbors** for WADI (k=15 for smaller SWaT dataset). Graph Attention Networks aggregate information: attention coefficients α_ij = softmax(LeakyReLU(a^T[Wh_i||Wh_j])), with aggregated representation h'_i = Σ(α_ij × h_j) for neighbors j. This captures physical relationships like upstream-downstream sensor dependencies, sensors in the same process stage, or shared control logic. STADN achieves F1=0.62 on WADI, demonstrating spatial modeling's importance for high-dimensional industrial systems.

**Physical process features** leverage domain knowledge about water distribution. Critical feature categories include water level trends in tanks and reservoirs, flow rate patterns through pumps and valves, pressure gradients across the distribution network, pump status transitions (on/off changes), valve position change patterns, and tank level states (low/normal/high thresholds). Research prioritizing flow indicators (FIT sensors), level transmitters (LT sensors), and pressure meters (PIT sensors) achieves better detection than treating all features equally, as these directly indicate hydraulic anomalies from attacks.

**Dimensionality reduction through PCA** provides lightweight alternative to autoencoder-based approaches. MAD-GAN documents that **first 8 principal components explain >95% variance** for WADI, while first 5 PCs cover >99.5% for SWaT. Reducing 127 features to 8 dimensions accelerates training by >33% while maintaining anomaly detection performance. The paper cautions that using only first 2 PCs yields insufficient recall (\<90%), but 5-8 components provide optimal balance. PCA advantages: simple, fast (0.05s computation), linear transformation, orthogonal features. Disadvantages: only captures linear relationships, may miss subtle anomalies in discarded components.

**Autoencoder dimensionality reduction** captures nonlinear patterns PCA misses. The Lightweight LSTM-VAE architecture for WADI uses: Input → LSTM(64) → Dense(32, mean) + Dense(32, log_var) → Latent(32) → Repeat(4) → LSTM(64) → Dense(112), totaling **81,584 parameters** for the small model or 384,752 for medium. Training time: 25 minutes for 14 epochs on WADI. The **latent dimension of 32-64** compresses 112 features while preserving temporal patterns through LSTM encoding. The variational approach with KL divergence regularization improves generalization over standard autoencoders. F1-score: 0.43 for WADI (baseline), significantly lower than graph-based approaches but with minimal computational footprint.

**Feature selection through ensemble methods** combines multiple statistical tests for robust minimal feature sets. The ELIDS approach fuses Chi-square, Information Gain, Gain Ratio, Correlation, Mutual Information, ANOVA, and ReliefF scores. This ensemble achieves **23.7% higher accuracy** than single feature selection methods and maintains performance across domains. For WADI, this typically selects **15-25 critical features** from the 112 post-filtered set, enabling ultra-lightweight deployment while maintaining 98-99% detection accuracy. The ensemble voting mechanism prevents poor selection from any single method's bias.

## Proper evaluation protocols prevent unrealistic performance estimates

**Standard k-fold cross-validation fundamentally violates temporal causality** in industrial control system data. Random shuffling breaks temporal dependencies and allows models to train on "future" data while predicting "past" observations—impossible in real deployment. Research on ICS datasets demonstrates cross-validation "systematically yields overly optimistic estimations" when temporal order is violated. Additionally, adjacent observations in time series exhibit high autocorrelation, causing test data to leak information into training folds through temporal dependencies even without explicit window overlap.

**Time-series cross-validation** preserves temporal ordering through approaches like TimeSeriesSplit (sklearn): training set grows incrementally where fold k trains on first k blocks and tests on block k+1. Blocked cross-validation divides data into contiguous blocks maintaining temporal sequence, testing each block using only prior blocks for training. Walk-forward validation most realistically simulates production deployment: train on t₀ to tₙ, test on tₙ₊₁ to tₙ₊ₘ, slide window forward. Critical parameter: **gap H between train and test** (typically 20-50 timesteps for WADI) prevents immediate autocorrelation leakage by removing observations adjacent to split boundary.

**hv-Block cross-validation** further enhances independence by blocking observations in each fold without shuffling and removing h observations adjacent to test set boundaries, creating temporal buffer zones. This "further increases independence among observations" according to time-series validation literature. Block size should match operational cycles (8-hour shifts, daily patterns), while gap size h should equal the embedding dimension or lag order of the temporal process.

**Train/validation/test split for WADI** follows consistent patterns across literature. Standard configuration: **95% of 14-day training period** for model training (13.3 days), **5% for validation** (0.7 days), then the separate **2-day test set** with attacks for final evaluation. Alternative ratio: 70% training, 20% validation, 10% test when splitting the full dataset. Critical requirements: strict temporal ordering where training data entirely precedes validation, which precedes test; temporal gap of 30-60 seconds between splits; and training set contains exclusively normal operation while test set mixes normal and attack periods.

**Threshold tuning must occur on validation set exclusively**. The proper protocol: (1) train model on training set, (2) generate anomaly scores on validation set, (3) select threshold maximizing F1-score or using 99th percentile of validation errors, (4) apply this fixed threshold to test set exactly once for final evaluation. Never iteratively adjust thresholds based on test set performance—this constitutes direct data leakage. DAICS framework sets threshold = T_base + max(predicted_errors_validation), while STADN uses maximum validation set anomaly score as threshold for test inference.

**Point-based versus range-based evaluation metrics** reveals critical distinction for operational ICS security. Point-based metrics (precision, recall, F1) treat each timestamp independently, creating several problems: attacks of different lengths receive unequal weighting (longer attacks easier to partially detect), detecting one point of a 100-point attack yields 1% recall despite operationally detecting the attack, and false positive rates appear artificially high when attack boundaries span multiple seconds of detection lag.

**Range-based F1 scoring** (Tatbul et al.) treats attacks as detected if ANY point within the attack window triggers an alarm. Range precision = correctly_detected_attack_ranges / total_detected_ranges, range recall = detected_attack_instances / total_attack_instances. CMU ESORICS study demonstrates range-F1 provides different optimal model selection than point-F1, with point-F1 potentially high even when multiple short attacks completely missed. **State-of-the-art papers report both metrics**: STADN achieves point-F1=0.62 and detects 14 of 15 attacks on WADI.

**Attack-wise evaluation** provides most actionable reporting for ICS operators. Best practice creates per-attack tables showing attack ID, duration, attack type, whether detected (yes/no), detection latency from attack start to first alarm, and percentage coverage of attack duration. Attack categories include single-point versus multi-point attacks, sensor spoofing versus actuator manipulation, and short (<5 minutes) versus long duration. WADI papers report "attacks detected: 14/15" or "13/15" as primary metric supplementing F1-scores, directly answering operators' question: "Will this system catch the attacks?"

**False alarm rate** matters critically for operator trust and operational feasibility. DAICS reports **5.3 interventions per hour** on WADI—potentially unsustainable for human operators. Better systems achieve <1 false alarm per hour while maintaining high detection rates. The metric calculation: FP/(FP+TN) as rate or FP per hour. Operational studies indicate **>5 false alarms per hour** causes alert fatigue where operators begin ignoring all alarms, defeating the IDS purpose entirely.

**Realistic performance expectations for WADI**: State-of-the-art F1-scores range **0.60-0.75**, dramatically lower than the >0.95 common on simpler datasets. STADN: F1=0.62, Precision=98.49%, Recall=45.57%. Lightweight LSTM-VAE: F1=0.43, Precision=46.98%, Recall=24.58%. Kravchik's 1D-CNN ensemble: F1=0.75 (best published). **Warning sign**: F1-scores >0.95 on WADI likely indicate data leakage rather than genuine breakthrough, requiring careful audit of preprocessing pipeline for information leakage.

## Optimization strategies enable deployment on resource-constrained edge devices

**Feature reduction maintains detection while dramatically reducing computational load**. The comprehensive protocol: (1) apply K-S test removing 14 unstable sensors, (2) eliminate 6 constant solenoid valves, (3) perform ensemble feature selection (Chi-square + Information Gain + Correlation + Mutual Information + ReliefF voting), (4) target **15-20 features** for balanced deployment or **7-10 for ultra-lightweight** edge nodes. Research demonstrates ensemble feature selection achieves **23.7% higher accuracy** than single methods while reducing features by 80-85%. This reduces WADI from 127 to 15-20 dimensions while maintaining 98-99% detection accuracy.

**Feature importance prioritization** from WADI domain analysis: **High priority (always include)**: flow indicators (FIT_*), level transmitters (LT_*), pressure meters (PIT_*), primary pump status, main valve positions. **Medium priority**: analyzer readings (AIT_*), secondary pumps, modulating valves. **Low priority (drop first)**: constant status indicators, redundant sensors with correlation >0.98, features failing K-S distribution tests. Process-aware selection focuses P1-W features on pump and storage, P2-W on consumer tank levels and flow rates, P3-W on return flow monitoring.

**Quantization provides massive compression** with minimal accuracy loss. The DNN-KDQ approach converts 32-bit floating-point models (**196.77 KB**) to 8-bit integers (**20.18 KB**), achieving **89.7% size reduction**. Inference time drops from 0.19ms to **0.07ms per sample**—a 63% speedup. Detection accuracy maintains **99.43%** post-quantization. Implementation: INT8 for weights and activations, preserve 32-bit bias for accuracy, use hardware acceleration on CPUs/GPUs/TPUs supporting integer operations, reduce memory access by ~90%. This enables deployment on microcontrollers and low-power ARM processors common in industrial environments.

**Knowledge distillation** transfers complex model capabilities to lightweight architectures. Process: (1) train teacher model (complex DNN) on full WADI dataset achieving high accuracy baseline with 64-128 neuron layers, (2) distill knowledge to student model (2-3 hidden layers, 32-64 neurons) using combined loss function of true labels plus softened teacher outputs with temperature scaling, (3) apply post-training quantization to INT8, yielding **90% size reduction** without retraining. The student model learns to mimic teacher's decision boundaries while requiring fraction of computational resources.

**Lightweight LSTM-VAE architectures** specifically designed for resource constraints achieve remarkable efficiency. The Small configuration: Input(batch, 4, 112) → LSTM(32) → Dense(16, mean) + Dense(16, log_var) → Latent(16) → LSTM(32) → Dense(112), totaling **81,584 parameters**. Medium configuration: similar architecture with 64-unit LSTMs and 32-dimensional latent space, **384,752 parameters**. Training time: **21-25 minutes** on standard GPU versus 16+ hours (8 GPUs) for competing methods. Model size: **81-400 KB** versus >1 MB for standard approaches. Performance: F1=0.43 on WADI, demonstrating that lightweight architectures remain competitive while enabling edge deployment.

**Preprocessing pipeline optimization** separates offline and online operations. **Offline (pre-deployment)**: perform K-S statistical tests, calculate normalization parameters (mean, std, min, max from training data), determine optimal window size (w=4 for WADI), set detection threshold (99th percentile validation errors), train and quantize model, validate on test set. **Online (real-time edge)**: extract only selected features (15-20 instead of 127), normalize using pre-computed parameters (single multiply-add per feature), maintain circular buffer for sliding window (4 samples × features), inference with frozen quantized model, no retraining. This division minimizes edge computation to feature extraction, normalization, and inference—all achievable in <1ms per sample.

**Memory-efficient windowing** using circular buffers maintains fixed-size buffer (4 samples for WADI with w=4) with O(1) update complexity, minimal allocation/deallocation overhead, and cache-friendly sequential access patterns. Feature-wise processing handles each sensor stream independently, enabling parallelization on multi-core edge processors and reducing peak memory by processing features in batches rather than loading entire window simultaneously.

**Edge hardware validation** confirms practical deployment feasibility. **Raspberry Pi 4** (ARM Cortex-A72, 4GB RAM): DNN-KDQ model achieves 0.07ms inference, 20.18 KB model size, sufficient for continuous WADI monitoring at 1Hz sampling. **NVIDIA Jetson Nano**: 0.185ms inference for binary classification, suitable for multi-sensor monitoring. **FPGA implementations**: sub-millisecond guaranteed latency with Data Flow Processing architecture, low silicon cost, high performance per watt. **ARM Cortex-M microcontrollers**: feasible with ultra-lightweight models (<100KB), suitable for distributed sensor nodes.

**Configuration profiles** guide deployment decisions. **Ultra-low power** (IoT edge nodes): 7-10 features, LW-LSTM-VAE-S (81K params), INT8, w=4, targets 95-97% accuracy, <1ms inference, <50KB memory. **Balanced** (Raspberry Pi/gateway): 15-20 features, DNN-KDQ or LW-LSTM-VAE-M, INT8, w=4, targets 98-99% accuracy, <0.1ms inference, <100KB memory. **Performance** (edge server): 25-30 features, full LSTM-VAE-M, INT16/FP32, w=4-8, targets >99% accuracy, <0.5ms inference, <500KB memory. The balanced configuration provides optimal tradeoff for most industrial deployments, maintaining high accuracy while fitting within typical gateway device constraints.

## Practical implementation roadmap for hierarchical IDS deployment

**Phase 1 preparation** begins with dataset acquisition (WADI A2 19 Nov 2019 version from iTrust SUTD), removal of first 21,600 samples (6-hour stabilization), and temporal split maintaining chronological order: 95% training (13.3 days), 5% validation (0.7 days), separate 2-day test set. Apply K-S test comparing training and validation distributions (threshold p<0.05), identifying typically 14 sensors for removal. Eliminate 6 constant solenoid valves. Calculate normalization parameters from training set only using StandardScaler or MinMaxScaler: store mean, std, min, max for deployment.

**Phase 2 feature engineering** performs ensemble feature selection combining correlation, mutual information, and statistical tests to identify top 15-25 features based on voting. For graph-based approaches, construct spatial dependency matrix using training data correlation, selecting top-k=40 neighbors. Create sliding windows (w=4 for lightweight or w=20 for performance) within each dataset partition independently, ensuring no windows cross split boundaries. Handle missing values by removing empty columns and imputing zeros or forward-fill for remainder.

**Phase 3 model development** trains chosen architecture on processed training data: LSTM-VAE for balanced accuracy-efficiency, graph neural networks (STADN approach) for state-of-the-art performance, or simple DNN for ultra-lightweight deployment. Implement early stopping based on validation loss. Tune detection threshold on validation set using 99th percentile of reconstruction errors or maximum anomaly score. Critical: never touch test set during this phase. For edge deployment, apply knowledge distillation from complex teacher to simple student model (50% parameter reduction), then post-training quantization to INT8 (90% size reduction).

**Phase 4 validation and deployment** evaluates model on test set exactly once, reporting both point-based metrics (Precision, Recall, F1) and range-based metrics (Range-F1, attacks detected/15, detection latency). Generate per-attack analysis table. Validate on target hardware: measure inference time (<1ms target), memory footprint (<100KB target), and power consumption. Deploy model with stored normalization parameters and detection threshold. Implement online monitoring: extract selected features only, normalize using pre-computed parameters, maintain circular buffer, generate anomaly scores, trigger alarms when threshold exceeded.

**Critical success factors** from literature: removing unstable features via K-S testing improves generalization by 15-20%, proper normalization using training-only statistics prevents 30-40% performance degradation from leakage, ensemble feature selection maintains accuracy while reducing features 80%, INT8 quantization enables edge deployment with <5% accuracy loss, and temporal validation splits prevent optimistic bias of 20-30% observed with shuffled cross-validation.

**GitHub repositories** providing practical implementations: elisejiuqizhang/USAD-on-WADI-and-SWaT (handles missing values and downsampling), d-ailin/GDN (graph deviation network with WADI version notes), astha-chem/mvts-ano-eval (comparative evaluation framework), and cbhua/swat-preprocess (similar preprocessing principles applicable to WADI). These repositories demonstrate end-to-end pipelines from raw data to trained models, valuable for reproducibility and understanding implementation details often omitted from papers.

The path from raw WADI A2 data to deployed hierarchical IDS requires rigorous attention to temporal causality, aggressive prevention of information leakage, and systematic optimization for target hardware constraints. Papers achieving F1-scores of 0.60-0.75 on WADI consistently apply these preprocessing principles, demonstrating that methodological rigor matters as much as architectural innovation for industrial anomaly detection systems. The combination of proper feature selection (127→15-20 features), temporal validation protocols, and edge optimization techniques (quantization, distillation) enables practical deployment of sophisticated intrusion detection on the resource-constrained infrastructure characteristic of industrial IoT environments.