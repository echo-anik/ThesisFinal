ALL 6 CONFIGURATIONS - REAL-WORLD MODEL NAMES
==============================================

QUICK REFERENCE TABLE
=====================

Config | Edge Algorithm      | Central Algorithm      | F1      | Esc% | Size_MB | Power_W
-------|-------------------|----------------------|---------|------|---------|--------
1      | RF (50, d15)       | RF (200, d30)        | 0.9872  | 4.1% | 2.16    | 2.117
2 ⭐   | RF (100, d20)      | GB (200, d7)         | 0.9827  | 1.6% | 5.90    | 2.204  BEST
3      | Extra Trees (50)    | RF (300, d30)        | 0.9867  | 3.9% | 4.20    | 2.137
4      | RF (50, d10)       | RF (200, d30)        | 0.9872  | 10.9%| 0.88    | 2.110
5      | RF (75, d15)       | GB (200, d7)         | 0.9825  | 3.3% | 3.28    | 2.151
6      | RF (100, d20)      | RF (300, d30)        | 0.9867  | 1.6% | 6.27    | 2.198


DETAILED SPECIFICATIONS
=======================

CONFIG_1_LIGHTWEIGHT
====================
Name: "Lightweight Random Forest Ensemble"

EDGE:
  Algorithm: RandomForestClassifier
  Trees: 50
  Max Depth: 15
  Min Samples Split: 10
  Min Samples Leaf: 5
  Class Weight: Balanced
  
CENTRAL:
  Algorithm: RandomForestClassifier
  Trees: 200
  Max Depth: 30
  Min Samples Split: 5
  Min Samples Leaf: 2
  Class Weight: Balanced

Performance:
  F1: 0.9872 (best edge performance!)
  Escalation: 4.1% (higher than Config_2)
  Model Size: 2.16 MB
  Power: 2.117 W

Thesis Description:
"Config_1 uses a 50-tree Random Forest on edge with a 200-tree 
Random Forest on central, achieving 0.9872 F1 with 4.1% escalation."


CONFIG_2_BALANCED ⭐ RECOMMENDED
================================
Name: "Balanced Random Forest + Gradient Boosting Hybrid"

EDGE:
  Algorithm: RandomForestClassifier
  Trees: 100
  Max Depth: 20
  Min Samples Split: 10
  Min Samples Leaf: 5
  Class Weight: Balanced

CENTRAL:
  Algorithm: GradientBoostingClassifier
  Boosting Rounds: 200
  Max Depth: 7
  Learning Rate: 0.1
  
Performance:
  F1: 0.9827 (production-ready)
  Escalation: 1.6-2.2% (LOWEST!)
  Model Size: 5.90 MB
  Power: 2.204 W

Thesis Description:
"Config_2 combines a 100-tree Balanced Random Forest on edge 
with a 200-round Gradient Boosting classifier on central, 
achieving 0.9827 F1 with only 1.6-2.2% escalation overhead."

Why Best: Perfect balance of accuracy, efficiency, and scalability


CONFIG_3_FASEDGE
================
Name: "Extra Trees Edge + Heavy Random Forest Central"

EDGE:
  Algorithm: ExtraTreesClassifier (Extremely Randomized Trees)
  Trees: 50
  Max Depth: 15
  Min Samples Split: 10
  Min Samples Leaf: 5
  Class Weight: Balanced

CENTRAL:
  Algorithm: RandomForestClassifier
  Trees: 300 (heaviest central)
  Max Depth: 30
  Min Samples Split: 5
  Min Samples Leaf: 2
  Class Weight: Balanced

Performance:
  F1: 0.9867 (very accurate!)
  Escalation: 3.9%
  Model Size: 4.20 MB
  Power: 2.137 W

Thesis Description:
"Config_3 pairs Extremely Randomized Trees (50 trees) on edge 
with a heavy 300-tree Random Forest on central, achieving 0.9867 F1 
with 3.9% escalation."

Note: Extra Trees faster training but less stable than RF


CONFIG_4_CONSERVATIVE
====================
Name: "Ultra-Lightweight Shallow Random Forest"

EDGE:
  Algorithm: RandomForestClassifier
  Trees: 50
  Max Depth: 10 (shallow!)
  Min Samples Split: 15 (high threshold)
  Min Samples Leaf: 8 (restrictive)
  Class Weight: Balanced

CENTRAL:
  Algorithm: RandomForestClassifier
  Trees: 200
  Max Depth: 30
  Min Samples Split: 5
  Min_samples Leaf: 2
  Class Weight: Balanced

Performance:
  F1: 0.9872 (same as Config_1)
  Escalation: 10.9% (HIGHEST - edge is weak!)
  Model Size: 0.88 MB (SMALLEST!)
  Power: 2.110 W (LOWEST!)

Thesis Description:
"Config_4 uses a minimal 50-tree Random Forest with shallow depth 
on edge and a 200-tree Random Forest on central, achieving 0.9872 F1 
with 10.9% escalation (minimal resources, maximum delegation)."

Trade-off: Best for ultra-resource-constrained devices
Cost: High escalation (10.9%) means heavy central load


CONFIG_5_OPTIMALBALANCE
=======================
Name: "Mid-Size Random Forest + Gradient Boosting"

EDGE:
  Algorithm: RandomForestClassifier
  Trees: 75
  Max Depth: 15
  Min Samples Split: 10
  Min Samples Leaf: 5
  Class Weight: Balanced

CENTRAL:
  Algorithm: GradientBoostingClassifier
  Boosting Rounds: 200
  Max Depth: 7
  Learning Rate: 0.1

Performance:
  F1: 0.9825 (very close to Config_2!)
  Escalation: 3.3%
  Model Size: 3.28 MB
  Power: 2.151 W

Thesis Description:
"Config_5 combines a 75-tree Random Forest on edge with a 
200-round Gradient Boosting classifier on central, achieving 
0.9825 F1 with 3.3% escalation."

Note: Very similar to Config_2 but with 75 vs 100 trees (minimal difference)


CONFIG_6_HIGHACCURACY
===================
Name: "Full-Power Random Forest Ensemble (Accuracy Focus)"

EDGE:
  Algorithm: RandomForestClassifier
  Trees: 100
  Max Depth: 20
  Min Samples Split: 5 (low threshold - complex trees)
  Min Samples Leaf: 2 (permissive - deep splits)
  Class Weight: Balanced

CENTRAL:
  Algorithm: RandomForestClassifier
  Trees: 300 (maximum!)
  Max Depth: 30
  Min Samples Split: 5
  Min Samples Leaf: 2
  Class Weight: Balanced

Performance:
  F1: 0.9867 (very accurate!)
  Escalation: 1.6% (tied with Config_2 for LOWEST!)
  Model Size: 6.27 MB (LARGEST!)
  Power: 2.198 W

Thesis Description:
"Config_6 employs a full-powered Random Forest (100 trees, depth 20) 
on edge with a maximum 300-tree Random Forest on central, achieving 
0.9867 F1 with 1.6% escalation (maximum accuracy focus)."

Trade-off: Best accuracy but largest models
Cost: +1.4MB over Config_2 for marginal F1 improvement (0.004)


COMPARISON & SELECTION MATRIX
==============================

Choose CONFIG_1 if:
  ✓ Want simplicity (all Random Forest)
  ✓ Don't mind 4.1% escalation
  ✓ Like interpretability (easier than boosting)

Choose CONFIG_2 if: ⭐ RECOMMENDED
  ✓ Want best balance (accuracy + efficiency + escalation)
  ✓ Can tolerate 5.9MB on edge
  ✓ Want production-ready system
  ✓ Prefer this for thesis

Choose CONFIG_3 if:
  ✓ Want to explore Extra Trees algorithm
  ✓ Have fast training important
  ✓ Don't mind complex central

Choose CONFIG_4 if:
  ✓ Extreme resource constraints (Pi Zero)
  ✓ Battery-powered edge device
  ✓ Can accept 10.9% escalation
  ✓ Absolute minimum model size crucial

Choose CONFIG_5 if:
  ✓ Want GB central but smaller edge
  ✓ 75 trees enough for your data
  ✓ Marginal trade-off acceptable

Choose CONFIG_6 if:
  ✓ Maximum accuracy paramount
  ✓ Resources unlimited
  ✓ Want best possible F1
  ✓ Can justify larger models


ALGORITHM EXPLANATIONS
======================

RandomForest (RF):
  - Ensemble of decision trees
  - Each tree independent
  - Final decision: majority vote
  - Fast inference, interpretable
  - Used in: Configs 1, 2, 4, 5, 6

ExtraTrees (Extra Randomized Trees):
  - Like RF but trees built randomly
  - Faster training, less stable
  - Used in: Config_3 edge only

GradientBoosting (GB):
  - Sequential tree building
  - Each tree corrects previous errors
  - More accurate but slower
  - Better pattern recognition
  - Used in: Configs 2, 5 central


PARAMETER MEANINGS
==================

n_estimators: Number of trees
  Lower (50): Faster, less accurate
  Higher (300): Slower, more accurate
  Sweet spot: 100-200

max_depth: Tree depth limit
  Lower (10): Shallow, underfitting risk
  Higher (30): Deep, overfitting risk
  Sweet spot: 15-20

min_samples_split: Samples required to split node
  Higher (15): Conservative, less splits
  Lower (5): Aggressive, more splits

min_samples_leaf: Samples required in leaf
  Higher (8): Larger leaves, less overfit
  Lower (2): Smaller leaves, more detailed

class_weight='balanced': Handles class imbalance
  Gives more weight to minority class (attacks)


FOR YOUR THESIS
===============

You now have complete specifications for all 6 configurations.

For publication, use this template:

"We evaluated six hierarchical IDS configurations:
- Config_1: 50-tree RF edge + 200-tree RF central
- Config_2: 100-tree RF edge + 200-round GB central (SELECTED)
- Config_3: 50-tree ET edge + 300-tree RF central
- Config_4: 50-tree shallow RF edge + 200-tree RF central
- Config_5: 75-tree RF edge + 200-round GB central
- Config_6: 100-tree RF edge + 300-tree RF central

Config_2 emerged as optimal, balancing accuracy (F1=0.9827), 
resource efficiency (5.9MB, 2.2W), and network efficiency (1.6% escalation)."


REPRODUCIBILITY CHECKLIST
=========================

For each configuration, you need:
  ✓ Exact algorithm (RF, ET, GB)
  ✓ Number of estimators/rounds
  ✓ Max depth
  ✓ Min samples split
  ✓ Min samples leaf
  ✓ Learning rate (if GB)
  ✓ Class weight setting
  ✓ Random state (42 for reproducibility)

All provided above! ✓


END OF DOCUMENT
==============
